{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustogerman/miniforge3/envs/DL-Project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "from contextlib import nullcontext\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from peft import LoraConfig\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import cast_training_params, compute_snr\n",
    "from diffusers.utils import check_min_version, convert_state_dict_to_diffusers, is_wandb_available\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.utils.torch_utils import is_compiled_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make our relative library imports work\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    pretrained_model_name_or_path: str = \"CompVis/stable-diffusion-v1-4\"\n",
    "    datasets = [\"clothes_dataset\", \"pixelart_dataset\", \"photograph_dataset\"]\n",
    "    learning_rate: float = 1e-05\n",
    "    max_train_samples: int = 1000\n",
    "    num_train_epochs: int = 20\n",
    "    checkpointing_steps: int = 500\n",
    "    train_batch_size: int = 1\n",
    "    resume_from_checkpoint: str = \"latest\"\n",
    "    output_dir: str = \"./model_downloads/mlp_blended\"\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    # This can be set to 'no', 'fp16', 'bf16' based on system config\n",
    "    mixed_precision: str = None\n",
    "    report_to: str = \"wandb\"\n",
    "    seed: int = 42\n",
    "\n",
    "    # New fields with their argparse defaults\n",
    "    revision: str = None\n",
    "    variant: str = None\n",
    "    dataset_config_name: str = None\n",
    "    train_data_dir: str = None\n",
    "    image_column: str = \"image\"\n",
    "    caption_column: str = \"text\"\n",
    "    validation_prompt: str = None\n",
    "    num_validation_images: int = 4\n",
    "    validation_epochs: int = 1\n",
    "    cache_dir: str = None\n",
    "    resolution: int = 512\n",
    "    center_crop: bool = False\n",
    "    random_flip: bool = False\n",
    "    num_train_epochs: int = 100\n",
    "    max_train_steps: int = None\n",
    "    learning_rate: float = 1e-4\n",
    "    scale_lr: bool = False\n",
    "    lr_scheduler: str = \"constant\"\n",
    "    lr_warmup_steps: int = 500\n",
    "    snr_gamma: float = None\n",
    "    use_8bit_adam: bool = False\n",
    "    allow_tf32: bool = False\n",
    "    dataloader_num_workers: int = 0\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.999\n",
    "    adam_weight_decay: float = 1e-2\n",
    "    adam_epsilon: float = 1e-08\n",
    "    max_grad_norm: float = 1.0\n",
    "    push_to_hub: bool = False\n",
    "    hub_token: str = None\n",
    "    prediction_type: str = None\n",
    "    hub_model_id: str = None\n",
    "    logging_dir: str = \"logs\"\n",
    "    local_rank: int = -1\n",
    "    checkpoints_total_limit: int = None\n",
    "    enable_xformers_memory_efficient_attention: bool = False\n",
    "    noise_offset: float = 0\n",
    "    rank: int = 4\n",
    "    gradient_checkpointing: bool = False\n",
    "\n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.report_to == \"wandb\":\n",
    "    if not is_wandb_available():\n",
    "        raise ImportError(\n",
    "            \"Make sure to install wandb if you want to use it for logging during training.\")\n",
    "\n",
    "    import wandb\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if CONFIG.seed is not None:\n",
    "    set_seed(CONFIG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/17/2024 17:35:23 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: mps\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator_project_config = ProjectConfiguration(\n",
    "    project_dir=CONFIG.output_dir,\n",
    "    logging_dir=CONFIG.output_dir\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    # gradient_accumulation_steps=CONFIG.gradient_accumulation_steps,\n",
    "    # mixed_precision=CONFIG.mixed_precision,\n",
    "    log_with=CONFIG.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    ")\n",
    "\n",
    "# Disable AMP for MPS.\n",
    "if torch.backends.mps.is_available():\n",
    "    accelerator.native_amp = False\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if CONFIG.output_dir is not None:\n",
    "        os.makedirs(CONFIG.output_dir, exist_ok=True)\n",
    "\n",
    "    # We are not pushing our model to HuggingFace Hub\n",
    "    # if CONFIG.push_to_hub:\n",
    "    #     repo_id = create_repo(\n",
    "    #         repo_id=CONFIG.hub_model_id or Path(CONFIG.output_dir).name,\n",
    "    #         exist_ok=True,\n",
    "    #         token=CONFIG.hub_token\n",
    "    #     ).repo_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load noise scheduler, tokenizer, and models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from diffusers.loaders import LoraLoaderMixin\n",
    "from models.StableDiffusion import StableDiffusion\n",
    "\n",
    "class GatedMoELayer(nn.Module):\n",
    "    def __init__(self, channels, num_experts):\n",
    "        super(GatedMoELayer, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # One expert and one gate per unet\n",
    "        self.gates = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, 33), # Go from `(batch_size, 4, 64, 64)` to `(batch_size, 4, 32, 32)`\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(channels, channels, 25), # Go from `(batch_size, 4, 32, 32)` to `(batch_size, 4, 8, 8)`\n",
    "                nn.SiLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(channels*8*8, 1),\n",
    "                nn.ReLU(),\n",
    "            ) for _ in range(self.num_experts)\n",
    "        ])\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(channels, channels, 1), # Preserve input shape\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(channels, channels, 1), # Preserve input shape\n",
    "                nn.SiLU()\n",
    "            ) for _ in range(self.num_experts)\n",
    "        ])\n",
    "\n",
    "    def forward(self, latent_unets):\n",
    "        # Pass the noise through each gate. Each gate output has shape `(batch_size, 1)`\n",
    "        gate_outputs = [self.gates[i](noise) for i, noise in enumerate(latent_unets)]\n",
    "        gate_outputs = torch.stack((*gate_outputs, ), dim=1) # shape: `(batch_size, n_experts, 1)`\n",
    "        gate_outputs = F.softmax(gate_outputs, dim=1) # Softmax across experts per example\n",
    "        \n",
    "        # Pass the noise through each expert. Each expert output has shape same as latent\n",
    "        expert_output = [self.experts[i](noise) for i, noise in enumerate(latent_unets)]\n",
    "        expert_output = torch.stack((*expert_output, ), dim=1)\n",
    "\n",
    "        # After stacking, the expert weights have shape (batch_size, n_experts, 4, 64, 64)\n",
    "        # So, we flatten them here so we can multiply by each gate. New shape: (batch_size, n_experts, 4*64*64)\n",
    "        expert_output = expert_output.view((latent_unets[0].shape[0], self.num_experts, -1))\n",
    "\n",
    "        # Combine the gated experts and reshape back to latent shape\n",
    "        predicted_noise = (gate_outputs * expert_output).sum(axis=1)\n",
    "        predicted_noise = predicted_noise.view(latent_unets[0].shape)\n",
    "\n",
    "        # Output shape (batch_size, 4, 64, 64). So, all UNets are combined by experts.\n",
    "        return predicted_noise\n",
    "\n",
    "\n",
    "class MLPBlendedUNet(StableDiffusion):\n",
    "    def __init__(self, model_path: str, lora_paths: list[str]):\n",
    "        super().__init__(model_path)\n",
    "\n",
    "        # Freeze parameters of models to save more memory\n",
    "        self.text_encoder.requires_grad_(False)\n",
    "        self.vae.requires_grad_(False)\n",
    "\n",
    "        # Load the default UNets\n",
    "        self.unets = nn.ModuleList([\n",
    "            UNet2DConditionModel.from_pretrained(\n",
    "                CONFIG.pretrained_model_name_or_path,\n",
    "                subfolder=\"unet\",\n",
    "                revision=CONFIG.revision,\n",
    "                variant=CONFIG.variant\n",
    "            )\n",
    "\n",
    "            for _ in lora_paths\n",
    "        ])\n",
    "\n",
    "        self.num_channels_latents: int = self.unet.config.in_channels\n",
    "        self.unet = None  # let it be garbage collected since we don't need the original unet any more\n",
    "\n",
    "        # Apply lora weights to each UNet\n",
    "        for u, lora_path in zip(self.unets, lora_paths):\n",
    "            # load lora weights\n",
    "            state_dict, network_alphas = LoraLoaderMixin.lora_state_dict(\n",
    "                lora_path, weight_name='pytorch_lora_weights.safetensors')\n",
    "            LoraLoaderMixin.load_lora_into_unet(\n",
    "                state_dict, network_alphas=network_alphas, unet=u)\n",
    "\n",
    "        # Freeze the unet parameters before adding adapters\n",
    "        for idx, u in enumerate(self.unets):\n",
    "            print(f\"Preparing UNet #{idx}\")\n",
    "            u.requires_grad_(False)\n",
    "\n",
    "            for param in u.parameters():\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "            if accelerator.mixed_precision == \"fp16\":\n",
    "                weight_dtype = torch.float16\n",
    "\n",
    "            elif accelerator.mixed_precision == \"bf16\":\n",
    "                weight_dtype = torch.bfloat16\n",
    "\n",
    "            unet_lora_config = LoraConfig(\n",
    "                r=CONFIG.rank,\n",
    "                lora_alpha=CONFIG.rank,\n",
    "                init_lora_weights=\"gaussian\",\n",
    "                target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "            )\n",
    "\n",
    "            # Add adapter and make sure the trainable params are in float32.\n",
    "            u.add_adapter(unet_lora_config)\n",
    "\n",
    "            if CONFIG.mixed_precision == \"fp16\":\n",
    "                # only upcast trainable parameters (LoRA) into fp32\n",
    "                cast_training_params(u, dtype=torch.float32)\n",
    "\n",
    "            if CONFIG.enable_xformers_memory_efficient_attention:\n",
    "                if is_xformers_available():\n",
    "                    import xformers\n",
    "\n",
    "                    xformers_version = version.parse(xformers.__version__)\n",
    "                    u.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "                    if xformers_version == version.parse(\"0.0.16\"):\n",
    "                        logger.warning(\n",
    "                            \"xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, please update xFormers to at least 0.0.17. See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.\"\n",
    "                        )\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"xformers is not available. Make sure it is installed correctly\")\n",
    "\n",
    "            if CONFIG.gradient_checkpointing:\n",
    "                u.enable_gradient_checkpointing()\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "        self.moe_layer = GatedMoELayer(\n",
    "            channels=4, # match channels of latent space\n",
    "            num_experts=3  # Learn a gate for each unet\n",
    "        )\n",
    "\n",
    "    def predict_noise(self, latent_inputs: torch.Tensor, timesteps: int, encoded_prompt: torch.Tensor):\n",
    "        unet_outputs = []\n",
    "\n",
    "        for unet in self.unets:\n",
    "            unet_output = unet(latent_inputs, timesteps, encoded_prompt, return_dict=False)[0]\n",
    "            unet_outputs.append(unet_output)\n",
    "\n",
    "        moe_output = self.moe_layer(unet_outputs)\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text = moe_output.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        return noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'timestep_spacing', 'sample_max_value', 'prediction_type', 'thresholding', 'clip_sample_range', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'timestep_spacing', 'sample_max_value', 'prediction_type', 'thresholding', 'clip_sample_range', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'force_upcast', 'norm_num_groups', 'latents_std', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_type', 'time_embedding_type', 'cross_attention_norm', 'projection_class_embeddings_input_dim', 'only_cross_attention', 'dual_cross_attention', 'time_embedding_act_fn', 'encoder_hid_dim_type', 'conv_out_kernel', 'dropout', 'class_embed_type', 'addition_time_embed_dim', 'attention_type', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'addition_embed_type', 'class_embeddings_concat', 'resnet_time_scale_shift', 'upcast_attention', 'encoder_hid_dim', 'timestep_post_act', 'num_attention_heads', 'transformer_layers_per_block', 'use_linear_projection', 'resnet_out_scale_factor', 'conv_in_kernel', 'num_class_embeds', 'time_cond_proj_dim', 'time_embedding_dim', 'addition_embed_type_num_heads', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_type', 'time_embedding_type', 'cross_attention_norm', 'projection_class_embeddings_input_dim', 'only_cross_attention', 'dual_cross_attention', 'time_embedding_act_fn', 'encoder_hid_dim_type', 'conv_out_kernel', 'dropout', 'class_embed_type', 'addition_time_embed_dim', 'attention_type', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'addition_embed_type', 'class_embeddings_concat', 'resnet_time_scale_shift', 'upcast_attention', 'encoder_hid_dim', 'timestep_post_act', 'num_attention_heads', 'transformer_layers_per_block', 'use_linear_projection', 'resnet_out_scale_factor', 'conv_in_kernel', 'num_class_embeds', 'time_cond_proj_dim', 'time_embedding_dim', 'addition_embed_type_num_heads', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_type', 'time_embedding_type', 'cross_attention_norm', 'projection_class_embeddings_input_dim', 'only_cross_attention', 'dual_cross_attention', 'time_embedding_act_fn', 'encoder_hid_dim_type', 'conv_out_kernel', 'dropout', 'class_embed_type', 'addition_time_embed_dim', 'attention_type', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'addition_embed_type', 'class_embeddings_concat', 'resnet_time_scale_shift', 'upcast_attention', 'encoder_hid_dim', 'timestep_post_act', 'num_attention_heads', 'transformer_layers_per_block', 'use_linear_projection', 'resnet_out_scale_factor', 'conv_in_kernel', 'num_class_embeds', 'time_cond_proj_dim', 'time_embedding_dim', 'addition_embed_type_num_heads', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_type', 'time_embedding_type', 'cross_attention_norm', 'projection_class_embeddings_input_dim', 'only_cross_attention', 'dual_cross_attention', 'time_embedding_act_fn', 'encoder_hid_dim_type', 'conv_out_kernel', 'dropout', 'class_embed_type', 'addition_time_embed_dim', 'attention_type', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'addition_embed_type', 'class_embeddings_concat', 'resnet_time_scale_shift', 'upcast_attention', 'encoder_hid_dim', 'timestep_post_act', 'num_attention_heads', 'transformer_layers_per_block', 'use_linear_projection', 'resnet_out_scale_factor', 'conv_in_kernel', 'num_class_embeds', 'time_cond_proj_dim', 'time_embedding_dim', 'addition_embed_type_num_heads', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "Loading unet.\n",
      "Loading unet.\n",
      "Loading unet.\n",
      "04/17/2024 17:35:26 - INFO - peft.tuners.tuners_utils - Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "04/17/2024 17:35:26 - INFO - peft.tuners.tuners_utils - Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "04/17/2024 17:35:26 - INFO - peft.tuners.tuners_utils - Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing UNet #0\n",
      "\n",
      "\n",
      "Preparing UNet #1\n",
      "\n",
      "\n",
      "Preparing UNet #2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPBlendedUNet(\n",
       "  (text_encoder): CLIPTextModel(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 768)\n",
       "        (position_embedding): Embedding(77, 768)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (vae): AutoencoderKL(\n",
       "    (encoder): Encoder(\n",
       "      (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (down_blocks): ModuleList(\n",
       "        (0): DownEncoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DownEncoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DownEncoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DownEncoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Attention(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (up_blocks): ModuleList(\n",
       "        (0-1): 2 x UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): UpDecoderBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2D(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Attention(\n",
       "            (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (to_out): ModuleList(\n",
       "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (unet): None\n",
       "  (unets): ModuleList(\n",
       "    (0-2): 3 x UNet2DConditionModel(\n",
       "      (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_proj): Timesteps()\n",
       "      (time_embedding): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (down_blocks): ModuleList(\n",
       "        (0): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DownBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (up_blocks): ModuleList(\n",
       "        (0): UpBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-2): 3 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=640, out_features=640, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=640, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=640, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "              (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=320, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=768, out_features=320, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default_0): Identity()\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default_0): Linear(in_features=320, out_features=4, bias=False)\n",
       "                          (default): Linear(in_features=320, out_features=4, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default_0): Linear(in_features=4, out_features=320, bias=False)\n",
       "                          (default): Linear(in_features=4, out_features=320, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2DCrossAttn(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default_0): Identity()\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (to_k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default_0): Identity()\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (to_v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default_0): Identity()\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default_0): Identity()\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (to_k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default_0): Identity()\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (to_v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default_0): Identity()\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default_0): Linear(in_features=768, out_features=4, bias=False)\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default_0): Identity()\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default_0): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default_0): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (moe_layer): GatedMoELayer(\n",
       "    (gates): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): Conv2d(4, 4, kernel_size=(33, 33), stride=(1, 1))\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(4, 4, kernel_size=(25, 25), stride=(1, 1))\n",
       "        (3): SiLU()\n",
       "        (4): Flatten(start_dim=1, end_dim=-1)\n",
       "        (5): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (6): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (experts): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): SiLU()\n",
       "        (2): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_scheduler: DDPMScheduler = DDPMScheduler.from_pretrained(\n",
    "    CONFIG.pretrained_model_name_or_path,\n",
    "    subfolder=\"scheduler\"\n",
    ")\n",
    "\n",
    "tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(\n",
    "    CONFIG.pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    "    revision=CONFIG.revision\n",
    ")\n",
    "\n",
    "# The blended UNet\n",
    "blended_unet = MLPBlendedUNet(\n",
    "    model_path=\"CompVis/stable-diffusion-v1-4\",\n",
    "    lora_paths=[\n",
    "        './model_downloads/clothes_finetuned_model',\n",
    "        './model_downloads/pixelart_finetuned_model',\n",
    "        './model_downloads/photograph_finetuned_model'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# For mixed precision training we cast all non-trainable weights (vae, non-lora text_encoder and non-lora unet) to half-precision\n",
    "# as these weights are only used for inference, keeping weights in full precision is not required.\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "# Move unet, vae and text_encoder to device and cast to weight_dtype\n",
    "blended_unet.to(accelerator.device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable TF32 for faster training on Ampere GPUs,\n",
    "# cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "if CONFIG.allow_tf32:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "if CONFIG.scale_lr:\n",
    "    CONFIG.learning_rate = (\n",
    "        CONFIG.learning_rate * CONFIG.gradient_accumulation_steps *\n",
    "        CONFIG.train_batch_size * accelerator.num_processes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "if CONFIG.use_8bit_adam:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
    "        )\n",
    "\n",
    "    optimizer_cls = bnb.optim.AdamW8bit\n",
    "else:\n",
    "    optimizer_cls = torch.optim.AdamW\n",
    "\n",
    "lora_layers = filter(lambda p: p.requires_grad, blended_unet.parameters())\n",
    "\n",
    "optimizer = optimizer_cls(\n",
    "    lora_layers,\n",
    "    lr=CONFIG.learning_rate,\n",
    "    betas=(CONFIG.adam_beta1, CONFIG.adam_beta2),\n",
    "    weight_decay=CONFIG.adam_weight_decay,\n",
    "    eps=CONFIG.adam_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(12566) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets: you can either provide your own training and evaluation files (see below)\n",
    "# or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "# In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
    "# download the dataset.\n",
    "# TODO: ALL DATASETS NEED TO BE LOADED\n",
    "if CONFIG.datasets[0] is not None:\n",
    "    # UPDATING THIS FROM ORIGINAL VERSION:\n",
    "    # We load from disk instead of using load_dataset\n",
    "    dataset = {'train': load_from_disk(CONFIG.datasets[0])}\n",
    "else:\n",
    "    data_files = {}\n",
    "\n",
    "    if CONFIG.train_data_dir is not None:\n",
    "        data_files[\"train\"] = os.path.join(CONFIG.train_data_dir, \"**\")\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"imagefolder\",\n",
    "        data_files=data_files,\n",
    "        cache_dir=CONFIG.cache_dir,\n",
    "    )\n",
    "    # See more about loading custom images at\n",
    "    # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize inputs and targets.\n",
    "column_names = dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME_MAPPING = {\n",
    "    \"lambdalabs/pokemon-blip-captions\": (\"image\", \"text\"),\n",
    "}\n",
    "\n",
    "# 6. Get the column names for input/target.\n",
    "# TODO: ALL DATASETS NEED TO BE LOADED\n",
    "dataset_columns = DATASET_NAME_MAPPING.get(CONFIG.datasets[0], None)\n",
    "\n",
    "if CONFIG.image_column is None:\n",
    "    image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "else:\n",
    "    image_column = CONFIG.image_column\n",
    "    if image_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--image_column' value '{CONFIG.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "if CONFIG.caption_column is None:\n",
    "    caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "else:\n",
    "    caption_column = CONFIG.caption_column\n",
    "    if caption_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--caption_column' value '{CONFIG.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# We need to tokenize input captions and transform the images.\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    captions = []\n",
    "    for caption in examples[caption_column]:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "        elif isinstance(caption, (list, np.ndarray)):\n",
    "            # take a random caption if there are multiple\n",
    "            captions.append(random.choice(caption) if is_train else caption[0])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "            )\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(CONFIG.resolution,\n",
    "                          interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        transforms.CenterCrop(\n",
    "            CONFIG.resolution) if CONFIG.center_crop else transforms.RandomCrop(CONFIG.resolution),\n",
    "        transforms.RandomHorizontalFlip(\n",
    "        ) if CONFIG.random_flip else transforms.Lambda(lambda x: x),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_model(model):\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    model = model._orig_mod if is_compiled_module(model) else model\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"]\n",
    "                               for example in examples])\n",
    "    pixel_values = pixel_values.to(\n",
    "        memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with accelerator.main_process_first():\n",
    "    if CONFIG.max_train_samples is not None:\n",
    "        # WE ALSO CHANGE THIS FROM ORIGINAL:\n",
    "        # dataset[\"train\"] = dataset[\"train\"].shuffle(seed=CONFIG.seed).select(range(CONFIG.max_train_samples))\n",
    "        dataset[\"train\"] = dataset[\"train\"].select(\n",
    "            range(CONFIG.max_train_samples))\n",
    "    # Set the training transforms\n",
    "    train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=CONFIG.train_batch_size,\n",
    "    num_workers=CONFIG.dataloader_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / CONFIG.gradient_accumulation_steps)\n",
    "if CONFIG.max_train_steps is None:\n",
    "    CONFIG.max_train_steps = CONFIG.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    CONFIG.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=CONFIG.lr_warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=CONFIG.max_train_steps * accelerator.num_processes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/17/2024 17:35:53 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "python(12567) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(12568) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfaustotnc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "python(12572) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(12573) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/faustogerman/Programming/GitHub/DiffusionOfExperts/wandb/run-20240417_173554-tkjgp7iv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/faustotnc/text2image-fine-tune/runs/tkjgp7iv' target=\"_blank\">vibrant-wind-13</a></strong> to <a href='https://wandb.ai/faustotnc/text2image-fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/faustotnc/text2image-fine-tune' target=\"_blank\">https://wandb.ai/faustotnc/text2image-fine-tune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/faustotnc/text2image-fine-tune/runs/tkjgp7iv' target=\"_blank\">https://wandb.ai/faustotnc/text2image-fine-tune/runs/tkjgp7iv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    blended_unet, optimizer, train_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / CONFIG.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    CONFIG.max_train_steps = CONFIG.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "CONFIG.num_train_epochs = math.ceil(\n",
    "    CONFIG.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if accelerator.is_main_process:\n",
    "    accelerator.init_trackers(\"text2image-fine-tune\", config=vars(CONFIG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/17/2024 17:35:56 - INFO - __main__ - ***** Running training *****\n",
      "04/17/2024 17:35:56 - INFO - __main__ -   Num examples = 1000\n",
      "04/17/2024 17:35:56 - INFO - __main__ -   Num Epochs = 100\n",
      "04/17/2024 17:35:56 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "04/17/2024 17:35:56 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "04/17/2024 17:35:56 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "04/17/2024 17:35:56 - INFO - __main__ -   Total optimization steps = 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 'latest' does not exist. Starting a new training run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "total_batch_size = CONFIG.train_batch_size * \\\n",
    "    accelerator.num_processes * CONFIG.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {CONFIG.num_train_epochs}\")\n",
    "logger.info(\n",
    "    f\"  Instantaneous batch size per device = {CONFIG.train_batch_size}\")\n",
    "logger.info(\n",
    "    f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(\n",
    "    f\"  Gradient Accumulation steps = {CONFIG.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {CONFIG.max_train_steps}\")\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "# initialize variables for keeping track of this expert's representative embedding\n",
    "total_embedding = 0\n",
    "num_samples = 0\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if CONFIG.resume_from_checkpoint:\n",
    "    if CONFIG.resume_from_checkpoint != \"latest\":\n",
    "        path = os.path.basename(CONFIG.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = os.listdir(CONFIG.output_dir)\n",
    "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "    if path is None:\n",
    "        accelerator.print(\n",
    "            f\"Checkpoint '{CONFIG.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "        )\n",
    "        CONFIG.resume_from_checkpoint = None\n",
    "        initial_global_step = 0\n",
    "    else:\n",
    "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "        accelerator.load_state(os.path.join(CONFIG.output_dir, path))\n",
    "        global_step = int(path.split(\"-\")[1])\n",
    "        checkpoint_representative_embedding = torch.load(os.path.join(\n",
    "            CONFIG.output_dir, path, 'representative_embedding.pt'))\n",
    "        num_samples = global_step\n",
    "        # checkpoint_representative_embedding starts out as an average\n",
    "        total_embedding = checkpoint_representative_embedding * num_samples\n",
    "\n",
    "        initial_global_step = global_step\n",
    "        first_epoch = global_step // num_update_steps_per_epoch\n",
    "else:\n",
    "    initial_global_step = 0\n",
    "\n",
    "progress_bar = tqdm(\n",
    "    range(0, CONFIG.max_train_steps),\n",
    "    initial=initial_global_step,\n",
    "    desc=\"Steps\",\n",
    "    # Only show the progress bar once on each machine.\n",
    "    disable=not accelerator.is_local_main_process,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 18.05 GB, other allocations: 78.52 MB, max allowed: 18.13 GB). Tried to allocate 20.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 53\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown prediction type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoise_scheduler\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprediction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Predict the noise residual and compute loss\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m model_pred \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_latents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CONFIG\u001b[38;5;241m.\u001b[39msnr_gamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(model_pred\u001b[38;5;241m.\u001b[39mfloat(), target\u001b[38;5;241m.\u001b[39mfloat(), reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 144\u001b[0m, in \u001b[0;36mMLPBlendedUNet.predict_noise\u001b[0;34m(self, latent_inputs, timesteps, encoded_prompt)\u001b[0m\n\u001b[1;32m    141\u001b[0m unet_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munets:\n\u001b[0;32m--> 144\u001b[0m     unet_output \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    145\u001b[0m     unet_outputs\u001b[38;5;241m.\u001b[39mappend(unet_output)\n\u001b[1;32m    147\u001b[0m moe_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoe_layer(unet_outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_condition.py:1281\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     upsample_size \u001b[38;5;241m=\u001b[39m down_block_res_samples[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(upsample_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m upsample_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[0;32m-> 1281\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mres_hidden_states_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupsample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     sample \u001b[38;5;241m=\u001b[39m upsample_block(\n\u001b[1;32m   1293\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39msample,\n\u001b[1;32m   1294\u001b[0m         temb\u001b[38;5;241m=\u001b[39memb,\n\u001b[1;32m   1295\u001b[0m         res_hidden_states_tuple\u001b[38;5;241m=\u001b[39mres_samples,\n\u001b[1;32m   1296\u001b[0m         upsample_size\u001b[38;5;241m=\u001b[39mupsample_size,\n\u001b[1;32m   1297\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_blocks.py:2542\u001b[0m, in \u001b[0;36mCrossAttnUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask, encoder_attention_mask)\u001b[0m\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2541\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[0;32m-> 2542\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2548\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py:397\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    385\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    386\u001b[0m             create_custom_forward(block),\n\u001b[1;32m    387\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    395\u001b[0m         )\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 397\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/diffusers/models/attention.py:392\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m     ff_output \u001b[38;5;241m=\u001b[39m _chunked_feed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff, norm_hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_size)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 392\u001b[0m     ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_zero\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    395\u001b[0m     ff_output \u001b[38;5;241m=\u001b[39m gate_mlp\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ff_output\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/diffusers/models/attention.py:664\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m     deprecate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, deprecation_message)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet:\n\u001b[0;32m--> 664\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/diffusers/models/activations.py:102\u001b[0m, in \u001b[0;36mGEGLU.forward\u001b[0;34m(self, hidden_states, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     deprecation_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     deprecate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, deprecation_message)\n\u001b[0;32m--> 102\u001b[0m hidden_states, gate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgelu(gate)\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/DL-Project/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 18.05 GB, other allocations: 78.52 MB, max allowed: 18.13 GB). Tried to allocate 20.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "for epoch in range(first_epoch, CONFIG.num_train_epochs):\n",
    "    unet.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(unet):\n",
    "            # Convert images to latent space\n",
    "            latents = blended_unet.vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype))\n",
    "            latents = latents.latent_dist.sample()\n",
    "            latents = latents * blended_unet.vae.config.scaling_factor\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            if CONFIG.noise_offset:\n",
    "                # https://www.crosslabs.org//blog/diffusion-with-offset-noise\n",
    "                noise += CONFIG.noise_offset * torch.randn((latents.shape[0], latents.shape[1], 1, 1), device=latents.device)\n",
    "\n",
    "            bsz = latents.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = blended_unet.text_encoder(batch[\"input_ids\"], return_dict=False)[0]\n",
    "\n",
    "            # representative embedding for this expert stable diffusor\n",
    "            batch_embedding = encoder_hidden_states.mean(dim=0)  # Average over sequence length\n",
    "            total_embedding += batch_embedding\n",
    "            num_samples += batch_embedding.shape[0]\n",
    "\n",
    "            # Get the target for loss depending on the prediction type\n",
    "            if CONFIG.prediction_type is not None:\n",
    "                # set prediction_type of scheduler if defined\n",
    "                noise_scheduler.register_to_config(prediction_type=CONFIG.prediction_type)\n",
    "\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "            # Predict the noise residual and compute loss\n",
    "            model_pred = unet.predict_noise(noisy_latents, timesteps, encoder_hidden_states)\n",
    "\n",
    "            if CONFIG.snr_gamma is None:\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "            else:\n",
    "                # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.\n",
    "                # Since we predict the noise instead of x_0, the original formulation is slightly changed.\n",
    "                # This is discussed in Section 4.2 of the same paper.\n",
    "                snr = compute_snr(noise_scheduler, timesteps)\n",
    "                mse_loss_weights = torch.stack([snr, CONFIG.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    mse_loss_weights = mse_loss_weights / snr\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    mse_loss_weights = mse_loss_weights / (snr + 1)\n",
    "\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "                loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n",
    "                loss = loss.mean()\n",
    "\n",
    "            # Gather the losses across all processes for logging (if we use distributed training).\n",
    "            avg_loss = accelerator.gather(loss.repeat(CONFIG.train_batch_size)).mean()\n",
    "            train_loss += avg_loss.item() / CONFIG.gradient_accumulation_steps\n",
    "\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "            if accelerator.sync_gradients:\n",
    "                params_to_clip = lora_layers\n",
    "                accelerator.clip_grad_norm_(params_to_clip, CONFIG.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "            accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "            train_loss = 0.0\n",
    "\n",
    "            if global_step % CONFIG.checkpointing_steps == 0:\n",
    "                if accelerator.is_main_process:\n",
    "                    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "                    if CONFIG.checkpoints_total_limit is not None:\n",
    "                        checkpoints = os.listdir(CONFIG.output_dir)\n",
    "                        checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "                        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "                        # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
    "                        if len(checkpoints) >= CONFIG.checkpoints_total_limit:\n",
    "                            num_to_remove = len(checkpoints) - CONFIG.checkpoints_total_limit + 1\n",
    "                            removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                            logger.info(f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\")\n",
    "                            logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n",
    "\n",
    "                            for removing_checkpoint in removing_checkpoints:\n",
    "                                removing_checkpoint = os.path.join(CONFIG.output_dir, removing_checkpoint)\n",
    "                                shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                    save_path = os.path.join(CONFIG.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "                    checkpoint_representative_embedding = total_embedding / num_samples\n",
    "                    torch.save(checkpoint_representative_embedding, os.path.join(save_path, 'representative_embedding.pt'))\n",
    "\n",
    "                    unwrapped_unet = unwrap_model(unet)\n",
    "                    unet_lora_state_dict = convert_state_dict_to_diffusers(\n",
    "                        get_peft_model_state_dict(unwrapped_unet)\n",
    "                    )\n",
    "\n",
    "                    StableDiffusionPipeline.save_lora_weights(\n",
    "                        save_directory=save_path,\n",
    "                        unet_lora_layers=unet_lora_state_dict,\n",
    "                        safe_serialization=True,\n",
    "                    )\n",
    "\n",
    "                    logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "        logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "\n",
    "        if global_step >= CONFIG.max_train_steps:\n",
    "            break\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if CONFIG.validation_prompt is not None and epoch % CONFIG.validation_epochs == 0:\n",
    "            logger.info(\n",
    "                f\"Running validation... \\n Generating {CONFIG.num_validation_images} images with prompt:\"\n",
    "                f\" {CONFIG.validation_prompt}.\"\n",
    "            )\n",
    "            # create pipeline\n",
    "            pipeline = DiffusionPipeline.from_pretrained(\n",
    "                CONFIG.pretrained_model_name_or_path,\n",
    "                unet=unwrap_model(unet),\n",
    "                revision=CONFIG.revision,\n",
    "                variant=CONFIG.variant,\n",
    "                torch_dtype=weight_dtype,\n",
    "            )\n",
    "            pipeline = pipeline.to(accelerator.device)\n",
    "            pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "            # run inference\n",
    "            generator = torch.Generator(device=accelerator.device)\n",
    "            if CONFIG.seed is not None:\n",
    "                generator = generator.manual_seed(CONFIG.seed)\n",
    "            images = []\n",
    "            if torch.backends.mps.is_available():\n",
    "                autocast_ctx = nullcontext()\n",
    "            else:\n",
    "                autocast_ctx = torch.autocast(accelerator.device.type)\n",
    "\n",
    "            with autocast_ctx:\n",
    "                for _ in range(CONFIG.num_validation_images):\n",
    "                    images.append(\n",
    "                        pipeline(\n",
    "                            CONFIG.validation_prompt, num_inference_steps=30, generator=generator).images[0]\n",
    "                    )\n",
    "\n",
    "            for tracker in accelerator.trackers:\n",
    "                if tracker.name == \"tensorboard\":\n",
    "                    np_images = np.stack([np.asarray(img) for img in images])\n",
    "                    tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\n",
    "                if tracker.name == \"wandb\":\n",
    "                    tracker.log(\n",
    "                        {\n",
    "                            \"validation\": [\n",
    "                                wandb.Image(\n",
    "                                    image, caption=f\"{i}: {CONFIG.validation_prompt}\")\n",
    "                                for i, image in enumerate(images)\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            del pipeline\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
