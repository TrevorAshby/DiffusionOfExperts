
Current Implementation:
1. during finetuning, average the encoded training text prompts into a 'representative embedding' for that expert model
2. at test time, get a cosine similarity of the encoded test-time text prompt and the representative embeddings for each expert model
3. perform softmax on these to normalize them in range 0-1. these will be the weights for the next step
4. for each timestep in the diffusion process, get a weighted average of the predicted noise for each expert's unet using the weights from step 3

Proposed Implementation:
(1) Load each finetuned U-Net into a module list.
(2) Create a Sequential layer that takes three inputs (one for each output of the U-Net) plus the embedding of the prompt. One output for the noise prediction.
(3) The first layer of this Sequential should be a layer norm, since the U-Net outputs and the embeddings might be in different scales.
(3) During forward, concatenate the finetuned U-Net outputs and the prompt embedding before passing to the sequential layer.
(4) Backward as usual through the sequential layer and (maybe) the UNets. We can also just freeze the UNets for speed. 